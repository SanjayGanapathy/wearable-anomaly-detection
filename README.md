# Health Anomaly Detection with Wearable Data

This project provides a backend service for detecting, ranking, and explaining physiological anomalies from intraday Fitbit data. It uses an Isolation Forest machine learning model and Google's Gemini Large Language Model (LLM) to translate complex data into human-readable explanations suitable for clinical researchers.

The system is designed as a flexible, scalable, and scientifically rigorous engine for research platforms like "Easy Ants", empowering researchers to monitor study participants effectively.

## Key Features

- **Multi-Day Analysis**: Processes continuous data over specified date ranges to capture long-term trends.
- **Flexible Anomaly Targeting**: Researchers can specify which metric to target for anomaly detection (e.g., `heart_rate`, `steps`), tailoring the analysis to their study.
- **Rich Contextual Data**: Integrates multiple data streams for a holistic analysis, including:
    - Intraday Heart Rate & Step Count
    - Sleep Stages (Deep, Light, REM, Awakenings)
    - Heart Rate Variability (HRV)
    - Participant-specific questionnaire data (e.g., exercise habits, caffeine use).
- **Scientific Validation**:
    - **Statistical Rigor**: Ranks anomalies using Z-scores for a quantitative measure of significance.
    - **A/B Testing Framework**: Justifies the use of a complex model by comparing it against a simple baseline, complete with an automated qualitative report generated by an LLM.
    - **Benchmarking**: Compares `IsolationForest` against other standard algorithms like `Local Outlier Factor` and `One-Class SVM`.
- **Automated Explanations**: Utilizes an LLM to generate clear, context-aware explanations for each detected anomaly.
- **API-Driven**: Exposes a Flask API endpoint for easy integration with web dashboards and other applications.
- **Unit Tested**: Includes a suite of unit tests for core modules to ensure code reliability.

## Project Structure

-   `config.py`: Central configuration for file paths, API keys, and model parameters.
-   `data_loader.py`: Handles loading, merging, and encoding of all data sources.
-   `feature_engineering.py`: Creates time-based and rolling-window features.
-   `anomaly_model.py`: Contains the Isolation Forest model for detecting and ranking anomalies.
-   `llm_explainer.py`: Interacts with the Google Gemini API to generate explanations.
-   `pipeline.py`: Orchestrates the entire workflow from data loading to explanation.
-   `app.py`: Runs the Flask web server and defines the API endpoints.
-   `tuner.py`: A utility script to help researchers tune the model's sensitivity.
-   `benchmarker.py`: A utility script to perform A/B tests and save anomaly results.
-   `compare_anomalies.py`: Uses an LLM to generate a qualitative report comparing the results of the A/B test.
-   `tests/`: Contains all unit tests to ensure code reliability.

## How to Use

### 1. Setup
Install the required Python packages:
```bash
pip install pandas scikit-learn google-generativeai Flask
```

### 2. Configuration
Open `config.py` and set the required variables, including your `GOOGLE_API_KEY` and the correct paths to your data files. Create a `questionnaire.csv` in the root directory.

### 3. Running the API Server
To start the anomaly detection service, run:
```bash
python app.py
```
The server will start and be accessible at `http://127.0.0.1:5000`.

### 4. API Documentation

**`GET /analyze_range`**

Triggers the full analysis pipeline for a specified date range.

**Query Parameters:**
-   `start_date` (required): The start of the date range in `YYYY-MM-DD` format.
-   `end_date` (required): The end of the date range in `YYYY-MM-DD` format.
-   `target` (optional): The feature to rank anomalies by. Defaults to `heart_rate`. Can also be `steps`.

**Example:**
```bash
curl "[http://127.0.0.1:5000/analyze_range?start_date=2025-07-01&end_date=2025-07-07&target=heart_rate](http://127.0.0.1:5000/analyze_range?start_date=2025-07-01&end_date=2025-07-07&target=heart_rate)"
```

### 5. Running the A/B Test & Comparison

To generate the report that justifies the complex model (as requested by your PI), run the following scripts in order:

**A. Run the A/B Test:**
This will create two files: `simple_model_anomalies.csv` and `complex_model_anomalies.csv`.
```bash
python benchmarker.py
```

**B. Generate the LLM Comparison Report:**
This reads the two CSV files and generates a qualitative report.
```bash
python compare_anomalies.py
```

### 6. Running Unit Tests
To verify that all components are working correctly, run the unit test suite:
```bash
python -m unittest discover
```